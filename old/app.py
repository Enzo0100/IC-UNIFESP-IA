import os
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# Importa√ß√µes do LangChain e outras bibliotecas
from langchain_community.document_loaders import UnstructuredWordDocumentLoader, UnstructuredExcelLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# --- CONFIGURA√á√ÉO E VARI√ÅVEIS GLOBAIS ---
load_dotenv()

# Para desenvolvimento, os IDs agora s√£o caminhos para arquivos locais
GOOGLE_DOC_ID = "docs.docx"
GOOGLE_SHEET_ID = "planilha.xlsx"

# Pasta onde o banco de dados vetorial ser√° salvo
VECTOR_STORE_PATH = "faiss_index"

# --- FUN√á√ïES CORE (L√≥gica do RAG) ---

def carregar_e_vetorizar_documentos():
    """
    Fun√ß√£o para carregar, dividir e vetorizar os documentos locais.
    Retorna um objeto de banco de dados vetorial (FAISS).
    """
    print("==> ETAPA 1: Carregamento e Vetoriza√ß√£o <==")
    try:
        print("[1.1] Carregando documentos de fontes locais...")
        doc_loader = UnstructuredWordDocumentLoader(GOOGLE_DOC_ID)
        sheet_loader = UnstructuredExcelLoader(GOOGLE_SHEET_ID, mode="elements")
        
        docs = doc_loader.load()
        docs.extend(sheet_loader.load())

        if not docs:
            raise ValueError("Nenhum documento foi carregado. Verifique os nomes e os caminhos dos arquivos.")
        
        print(f"[1.2] Sucesso: {len(docs)} documentos carregados.")
        
        print("[1.3] Dividindo documentos em chunks...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        splits = text_splitter.split_documents(docs)
        print(f"[1.4] Sucesso: {len(splits)} chunks criados.")
        
        print("[1.5] Inicializando modelo de embedding do Google (Gemini)...")
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        print("[1.6] Sucesso: Modelo de embedding do Google pronto para uso.")
        
        print("[1.7] Criando banco de dados vetorial FAISS a partir dos chunks...")
        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        print("[1.8] Sucesso: Banco de dados vetorial criado.")

        vectorstore.save_local(VECTOR_STORE_PATH)
        print(f"[1.9] Sucesso: Banco de dados vetorial salvo em '{VECTOR_STORE_PATH}'.")
        print("==> ETAPA 1 CONCLU√çDA <==")
        return vectorstore
    except Exception as e:
        print(f"!!! ERRO na ETAPA 1: {e}")
        raise

def inicializar_chatbot():
    """
    Carrega o banco de dados vetorial (ou o cria se n√£o existir) e
    inicializa a cadeia de Pergunta e Resposta (RAG).
    """
    print("\n==> ETAPA 2: Inicializa√ß√£o do Chatbot <==")
    vectorstore = None
    if os.path.exists(VECTOR_STORE_PATH):
        try:
            print(f"[2.1] Encontrado banco de dados vetorial. Carregando de '{VECTOR_STORE_PATH}'...")
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            vectorstore = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
            print("[2.2] Sucesso: Banco de dados vetorial carregado.")
        except Exception as e:
            print(f"!!! ERRO ao carregar o √≠ndice FAISS existente: {e}")
            print("!!! O √≠ndice pode estar corrompido ou incompat√≠vel. Recriando do zero...")
            vectorstore = carregar_e_vetorizar_documentos()
    else:
        print("[2.1] Nenhum banco de dados vetorial encontrado. Iniciando processo de cria√ß√£o...")
        vectorstore = carregar_e_vetorizar_documentos()

    print("[2.3] Inicializando o modelo LLM (Google Gemini)...")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0)
    print("[2.4] Sucesso: Modelo LLM carregado.")

    print("[2.5] Criando a cadeia de RAG com um retriever otimizado...")

    # Usando Maximal Marginal Relevance (MMR) para a busca.
    # Isso garante que os resultados sejam relevantes para a pergunta, mas tamb√©m
    # diversificados, evitando trechos muito repetitivos ou redundantes.
    # 'k' √© o n√∫mero de documentos a serem retornados.
    # 'fetch_k' √© o n√∫mero de documentos a serem buscados inicialmente para o MMR reclassificar.
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={'k': 5, 'fetch_k': 20}
    )

    # O chain_type "stuff" √© eficiente e funciona bem com modelos de grande contexto como o Gemini.
    # Ele "enfia" (stuffs) todos os trechos recuperados diretamente no prompt.

    # --- CRIA√á√ÉO DE PROMPT CUSTOMIZADO ---
    # Este prompt guia o LLM sobre como se comportar e como usar o contexto.
    prompt_template = """
    Voc√™ √© um assistente especialista. Use os seguintes trechos de contexto para responder √† pergunta do usu√°rio.
    Se a resposta n√£o estiver no contexto, diga que voc√™ n√£o encontrou a informa√ß√£o nos documentos. N√£o tente inventar uma resposta.
    Seja claro e responda apenas com base nas informa√ß√µes dos documentos.

    Contexto:
    {context}

    Pergunta:
    {question}

    Resposta √∫til:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    print("[2.6] Sucesso: Cadeia de RAG criada.")
    
    # Armazena a cadeia no cache global para reuso
    cache["qa_chain"] = qa_chain
    print("[2.7] Sucesso: Cadeia armazenada em cache.")
    print("==> ETAPA 2 CONCLU√çDA. API pronta para receber requisi√ß√µes. <==")

# --- GERENCIAMENTO DO CICLO DE VIDA DA API ---

@asynccontextmanager
async def lifespan(app: FastAPI):
    # C√≥digo que roda na inicializa√ß√£o da API
    print("Iniciando a API...")
    inicializar_chatbot()
    yield
    # C√≥digo que roda no encerramento (limpeza)
    cache.clear()
    print("Cache limpo. API encerrada.")

# --- INICIALIZA√á√ÉO DA API FASTAPI ---

app = FastAPI(
    title="Chatbot RAG API",
    description="Uma API para fazer perguntas a documentos usando RAG e LLMs, com um frontend integrado.",
    version="1.1.0",
    lifespan=lifespan
)

# --- MIDDLEWARE (CORS) ---
# Permite que o frontend acesse a API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Em produ√ß√£o, restrinja para o seu dom√≠nio
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- MODELOS DE DADOS (Pydantic) ---

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    answer: str
    source_documents: list[dict]

# --- ENDPOINTS DA API ---

@app.get("/", tags=["Status"])
def read_root():
    """Endpoint de status para verificar se a API est√° no ar."""
    return {"status": "online", "message": "Bem-vindo √† API do Chatbot RAG!"}

@app.post("/ask", response_model=QueryResponse, tags=["Chatbot"])
async def ask_question(request: QueryRequest):
    """
    Recebe uma pergunta e retorna a resposta baseada nos documentos.
    """
    if "qa_chain" not in cache:
        raise HTTPException(status_code=503, detail="Servi√ßo indispon√≠vel. A cadeia de RAG n√£o foi inicializada.")

    print(f"Recebida pergunta: {request.query}")
    
    try:
        # Invoca a cadeia com a pergunta
        response = cache["qa_chain"].invoke({"query": request.query})
        
        # Formata os documentos fonte para a resposta
        sources = [
            {"content": doc.page_content, "metadata": doc.metadata}
            for doc in response["source_documents"]
        ]

        return QueryResponse(answer=response["result"], source_documents=sources)
    except Exception as e:
        print(f"Erro ao processar a pergunta: {e}")
        raise HTTPException(status_code=500, detail="Ocorreu um erro interno ao processar sua pergunta.")
    

@app.post("/ask", response_model=QueryResponse, tags=["Chatbot"])
async def ask_question(request: QueryRequest):
    """
    Recebe uma pergunta e retorna a resposta baseada nos documentos,
    exceto quando for small talk (sem consultar a base de dados).
    """
    # classifica antes de qualquer chamada ao RAG
    intent = classificar_intencao(request.query)

    if intent == "smalltalk":
        # resposta r√°pida sem tocar no vetor
        resposta = (
            "Oi! üëã\n"
            "Posso te ajudar a consultar os documentos quando quiser. "
            "Me pergunte algo como: ‚Äúqual √© o prazo do projeto?‚Äù ou ‚Äúresuma a se√ß√£o X do arquivo‚Äù."
        )
        return QueryResponse(answer=resposta, source_documents=[])

    # a partir daqui √© pergunta de conte√∫do -> usa RAG
    if "qa_chain" not in cache:
        raise HTTPException(status_code=503, detail="Servi√ßo indispon√≠vel. A cadeia de RAG n√£o foi inicializada.")

    print(f"Recebida pergunta: {request.query}")
    try:
        response = cache["qa_chain"].invoke({"query": request.query})
        sources = [
            {"content": doc.page_content, "metadata": doc.metadata}
            for doc in response["source_documents"]
        ]
        return QueryResponse(answer=response["result"], source_documents=sources)
    except Exception as e:
        print(f"Erro ao processar a pergunta: {e}")
        raise HTTPException(status_code=500, detail="Ocorreu um erro interno ao processar sua pergunta.")


@app.post("/reindex", tags=["Administra√ß√£o"])
async def reindex_documents(background_tasks: BackgroundTasks):
    """
    For√ßa a releitura e vetoriza√ß√£o dos documentos do Google Drive em segundo plano.
    Use este endpoint se voc√™ atualizou os documentos fonte.
    """
    print("Requisi√ß√£o de reindexa√ß√£o recebida. O processo rodar√° em segundo plano.")
    background_tasks.add_task(inicializar_chatbot)
    return {
        "status": "success",
        "message": "A reindexa√ß√£o foi iniciada em segundo plano. A API continuar√° respondendo com a vers√£o antiga dos dados at√© que o processo termine."
    }
